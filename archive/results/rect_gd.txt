%%%%%%%%%%%%%%%%%%%
optimal_weight = 1.732080773549489
optimal_loss = -0.362794941111205
distance from sqrt3 = -2.996598061222322e-05

%%%%%%%%%%%%%%%%%%
loss =  -0.332335962358214
        -0.358441314479330
        -0.362570451958973
        -0.362793918972471
        -0.362794941111205
gradient = 
        -0.122362660970432
        -0.034960435054532
        -0.006751487687692
        -0.000434413971295
        -0.000019169640139
gradient_norm = 
        0.122362660970432
        0.034960435054532
        0.006751487687692
        0.000434413971295
        0.000019169640139
step = 
        2.898030435192289
        5.975849131262740
        9.414996588459305
        10.984311587958615
        11.205244845137285
fdd = 
        0.345061938569202
        0.167340235343039
        0.106213527599763
        0.091038932389376
        0.089243922272164
time = 
   1.0e+02 *
        1.532658551950000
        3.777087273360000
        4.393064380970000
        4.528748274860000
        4.478276975120000
weight = 
        1.454610715623428
        1.663529001072619
        1.727094234619267
        1.731865973038137
        1.732080773549489

% Raw Output %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
cparams = 
             maxiter: 100
              report: 1
                 eps: 1.000000000000000e-04
              hspace: 0.010000000000000
     line_search_eps: 1.000000000000000e-06
    line_search_beta: 0.500000000000000

TEST: true rectangle objective
iter: 1, loss: -3.32e-01, grad: 1.22e-01, 2nd-deri: 3.38e-01 
Current weight 1.46162970632087
iter: 2, loss: -3.59e-01, grad: 3.38e-02, 2nd-deri: 1.66e-01 
Current weight 1.66545587276798
iter: 3, loss: -3.63e-01, grad: 6.53e-03, 2nd-deri: 1.06e-01 
Current weight 1.72726965823175
iter: 4, loss: -3.63e-01, grad: 4.40e-04, 2nd-deri: 9.17e-02 
Current weight 1.73206278482079
iter: 5, loss: -3.63e-01, grad: 2.40e-06, 2nd-deri: 9.07e-02 
Current weight 1.73206278482711
gradient descent converged after 6 steps
optimal weight: 1.73e+00 
optimal value: -3.63e-01 
Now we do the actual optimization
iter: 1, loss: -3.32e-01, grad: 1.22e-01, 2nd-deri: 3.45e-01 
Current weight 1.45461071562343
iter: 2, loss: -3.58e-01, grad: 3.50e-02, 2nd-deri: 1.67e-01 
Current weight 1.66352900107262
iter: 3, loss: -3.63e-01, grad: 6.75e-03, 2nd-deri: 1.06e-01 
Current weight 1.72709423461927
iter: 4, loss: -3.63e-01, grad: 4.34e-04, 2nd-deri: 9.10e-02 
Current weight 1.73186597303814
iter: 5, loss: -3.63e-01, grad: 1.92e-05, 2nd-deri: 8.92e-02 
Current weight 1.73208077354949
gradient descent converged after 6 steps
optimal weight: 1.73e+00 
optimal value: -3.63e-01 

%%%%%%%%%%%%%%%%%%%
optimal_weight = 1.732080773549489
distance from sqrt3 = -2.996598061222322e-05
loss =  -0.332335962358214
        -0.358441314479330
        -0.362570451958973
        -0.362793918972471
        -0.362794941111205
gradient = 
        -0.122362660970432
        -0.034960435054532
        -0.006751487687692
        -0.000434413971295
        -0.000019169640139
gradient_norm = 
        0.122362660970432
        0.034960435054532
        0.006751487687692
        0.000434413971295
        0.000019169640139
step = 
        2.898030435192289
        5.975849131262740
        9.414996588459305
        10.984311587958615
        11.205244845137285
fdd = 
        0.345061938569202
        0.167340235343039
        0.106213527599763
        0.091038932389376
        0.089243922272164
time = 
   1.0e+02 *
        1.532658551950000
        3.777087273360000
        4.393064380970000
        4.528748274860000
        4.478276975120000
weight = 
        1.454610715623428
        1.663529001072619
        1.727094234619267
        1.731865973038137
        1.732080773549489